Atlas Analysis Project, designed to explore particle physics through the analysis of data from the ATLAS experiment
 at the Large Hadron Collider (LHC).
This project leverages the ATLAS Open Data, which provides access to simulated data
sets that emulate the real data collected by the ATLAS detector. 
The primary focus of this project is to analyze events potentially related to the Higgs boson.


`load_data.py`

Purpose: This script is designed to load the ATLAS Open Data necessary for the Higgs boson analysis from `.root` files hosted online. It supports the initial phase of the data processing pipeline which involves accessing and reading large datasets used in high-energy physics.

Functionality:
-Data Loading: The script uses the `uproot` library to access and read data from `.root` files, which are commonly used in particle physics for storing complex data structures.
- Sample Handling: It processes several data samples, including actual collision data and simulated backgrounds, organized into categories like real data events, background processes (`Z,t\bar{t}` and `ZZ^*`), and Higgs boson signal simulations. 
(The original HZZAnalysis.ipynb notebook uses LaTeX formatting to accurately represent particle physics terms, which enhances readability and educational value within Jupyter Notebook interfaces that support LaTeX. Examples include labels like $Z, t\bar{t}$ and $ZZ^*$ for different data samples.sample labels have been simplified to ensure compatibility with environments that do not support LaTeX formatting (e.g., command line interfaces, some text editors). This modification aids in maintaining the readability of logs and error messages generated by the script.)

- Configuration:
  - Luminosity is set to 10 fb-1 to match the scale of the dataset.
  - Paths to the data files are defined to point to the official ATLAS Open Data repository.

Usage:
```
python load_data.py
```
This command executes the script, which processes the listed data samples and prints a status update for each.

---

`process_data.py` 

Purpose :
`process_data.py` is designed to process and filter particle physics data obtained from the ATLAS experiment, specifically focusing on events that are indicative of the Higgs boson decay into four leptons. This script is a critical component of the analysis workflow, as it prepares the data for further analysis and visualization.

Functionality and Steps:

1. Data Input: 
   - The script assumes data is loaded from `.root` files containing information about particle collision events. This data includes various properties of particles such as lepton type, charge, and kinematic measurements like transverse momentum (`pt`), pseudorapidity (`eta`), azimuthal angle (`phi`), and energy (`E`).

2.Filtering Events:
   - Four Leptons: Initially, the script filters events to include only those that have exactly four leptons. This criterion is based on the `lep_n` field from the dataset which indicates the number of leptons in each event.
3.Calculations:
   - Invariant Mass: For each event that passes the filters, the script calculates the invariant mass of the four-lepton system. This metric is crucial for identifying potential Higgs boson events, as its mass peak around 125 GeV/cÂ² is a signature of the Higgs boson.

4. Output :
   - The script outputs a filtered dataset, where each entry corresponds to an event likely relevant to Higgs boson studies. Each entry includes updated fields, such as the calculated invariant mass (`m4l`).

Usage:
- To run the script standalone, ensure it is executed in an environment where `numpy`, `awkward`, and `uproot` are installed. The script can be executed after `load_data.py`, which loads the `.root` files into the Python environment.

Example Command:
```
python process_data.py
```
---
`calculate_metrics.py`

Purpose:
`calculate_metrics.py` focuses on computing derived quantities and metrics from the processed dataset, particularly calculating the invariant mass from particle physics events and applying further selection criteria based on physical parameters to refine the dataset for subsequent analysis steps.

Functionality and Steps:

1. Data Input:
   - Receives data from `process_data.py`, which includes particle collision events pre-filtered for potential Higgs boson decays.

2. Metric Calculation:
   - Calculates additional metrics such as invariant mass and other relevant properties that are crucial for distinguishing between signal (potential Higgs boson events) and background noise.

3. Event Selection:
   - Applies additional selection criteria to isolate events that most likely represent the decay processes of interest. This step refines the dataset by focusing on events within specific ranges of calculated metrics, like the invariant mass window around 125 GeV.

4.Output:
   - Outputs a dataset where each event has passed all specified criteria for Higgs boson identification, making it suitable for detailed statistical analysis and visualization.

Usage:
- This script is designed to be run after `load_data.py` and `process_data.py`. It relies on their outputs as inputs for its calculations.
- Ensure that all dependencies are installed and data files are properly configured before execution.

Example Command:
```
python calculate_metrics.py
```
---

`visualization_script.py`

Purpose:
Designed to visualize the data processed by the earlier scripts, `visualization_script.py` creates histograms and other graphical representations to help in the analysis and interpretation of the data related to the Higgs boson.

Functionality:

1. Data Input:
   - Uses datasets prepared by `calculate_metrics.py` which include processed and filtered event data.

2. Visualization Techniques:
   - Generates histograms of key metrics like lepton transverse momentum and invariant mass distributions.
   - Uses libraries such as `matplotlib` for plotting, providing a visual insight into the data which is crucial for both qualitative and quantitative analysis.

3. Output:
   - Produces plots and graphics that highlight significant data trends and characteristics. These visual outputs are essential for presentations, reports, and further analytical tasks.

Usage:
- This script is typically run as the final step in the data analysis pipeline.
- It requires a graphical environment to display plots or capabilities to save plots to files.

Example Command:
```
python visualization_script.py
```
---

Docker Integration

To facilitate ease of deployment and ensure consistency across different computing environments, a `Dockerfile` is provided. This Dockerfile packages the application with all its dependencies into a container, which can be built and run locally or on any system supporting Docker.

Steps to Use Docker:
1. Build the Docker image:
   ```
   docker build -t atlas-analysis .
   ```
2. Run the container:
   ```
   docker run -p 80:80 atlas-analysis
   ```

This setup encapsulates the Python environment and scripts, ensuring that the application runs consistently regardless of the host system's configuration.
